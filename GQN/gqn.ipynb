{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural scene representation and rendering\n",
    "### https://deepmind.com/blog/neural-scene-representation-and-rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets: https://github.com/deepmind/gqn-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets Translater: https://github.com/l3robot/gqn_datasets_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from pixyz.distributions import Normal\n",
    "from pixyz.losses import NLL, KullbackLeibler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from shepardmetzler import ShepardMetzler, Scene, transform_viewpoint\n",
    "from conv_lstm import Conv2dLSTMCell\n",
    "\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tower or pool\n",
    "class Representation(nn.Module):\n",
    "    def __init__(self, n_channels, v_dim, r_dim=256, pool=True):\n",
    "        super(Representation, self).__init__()\n",
    "        # dimention of r\n",
    "        self.r_dim = k = r_dim\n",
    "        # pool: True, tower: False:\n",
    "        self.pool = pool\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_channels, k, kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(k, k, kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(k, k//2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(k//2, k, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(k + v_dim, k, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(k + v_dim, k//2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv7 = nn.Conv2d(k//2, k, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(k, k, kernel_size=1, stride=1)\n",
    "\n",
    "        self.avgpool  = nn.AvgPool2d(k//16)\n",
    "\n",
    "    def forward(self, x, v):\n",
    "        # Increase dimensions\n",
    "        v = v.view(v.size(0), -1, 1, 1)\n",
    "        v = v.repeat(1, 1, self.r_dim // 16, self.r_dim // 16)\n",
    "\n",
    "        # First skip-connected conv block\n",
    "        skip_in  = F.relu(self.conv1(x))\n",
    "        skip_out = F.relu(self.conv2(skip_in))\n",
    "\n",
    "        x = F.relu(self.conv3(skip_in))\n",
    "        x = F.relu(self.conv4(x)) + skip_out\n",
    "\n",
    "        # Second skip-connected conv block (merged)\n",
    "        skip_in = torch.cat([x, v], dim=1)\n",
    "        skip_out  = F.relu(self.conv5(skip_in))\n",
    "\n",
    "        x = F.relu(self.conv6(skip_in))\n",
    "        x = F.relu(self.conv7(x)) + skip_out\n",
    "\n",
    "        r = F.relu(self.conv8(x))\n",
    "\n",
    "        if self.pool:\n",
    "            r = self.avgpool(r)\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceCore(nn.Module):\n",
    "    def __init__(self, x_dim, v_dim, r_dim, h_dim):\n",
    "        super(InferenceCore, self).__init__()\n",
    "        self.core = Conv2dLSTMCell(2*h_dim + x_dim + v_dim + r_dim, h_dim, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "    def forward(self, x, v, r, h_g, h_e, c_e, u):\n",
    "        h_e, c_e = self.core(torch.cat([h_g, u, x, v, r], dim=1), [h_e, c_e])\n",
    "        return h_e, c_e\n",
    "    \n",
    "class GeneratorCore(nn.Module):\n",
    "    def __init__(self, v_dim, r_dim, z_dim, h_dim, SCALE):\n",
    "        super(GeneratorCore, self).__init__()\n",
    "        self.core = Conv2dLSTMCell(v_dim + r_dim + z_dim, h_dim, kernel_size=5, stride=1, padding=2)\n",
    "        self.upsample = nn.ConvTranspose2d(h_dim, h_dim, kernel_size=SCALE, stride=SCALE, padding=0)\n",
    "        \n",
    "    def forward(self, z, v, r, h_g, c_g, u):\n",
    "        h_g, c_g =  self.core(torch.cat([z, v, r], dim=1), [h_g, c_g])\n",
    "        u = self.upsample(h_g) + u\n",
    "        return h_g, c_g, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference(Normal):\n",
    "    def __init__(self, z_dim, h_dim):\n",
    "        super(Inference, self).__init__(cond_var=[\"h_i\"],var=[\"z\"])\n",
    "        self.z_dim = z_dim\n",
    "        self.eta_e = nn.Conv2d(h_dim, 2*z_dim, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "    def forward(self, h_i):\n",
    "        mu, logvar = torch.split(self.eta_e(h_i), self.z_dim, dim=1)\n",
    "        std = F.softplus(logvar)\n",
    "        return {\"loc\":mu, \"scale\":std}\n",
    "    \n",
    "class Prior(Normal):\n",
    "    def __init__(self, z_dim, h_dim):\n",
    "        super(Prior, self).__init__(cond_var=[\"h_g\"],var=[\"z\"])\n",
    "        self.z_dim = z_dim\n",
    "        self.eta_pi = nn.Conv2d(h_dim, 2*z_dim, kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "    def forward(self, h_g):\n",
    "        mu, logvar = torch.split(self.eta_pi(h_g), self.z_dim, dim=1)\n",
    "        std = F.softplus(logvar)\n",
    "        return {\"loc\":mu ,\"scale\":std}\n",
    "    \n",
    "class Generator(Normal):\n",
    "    def __init__(self, x_dim, h_dim):\n",
    "        super(Generator, self).__init__(cond_var=[\"u\", \"sigma\"],var=[\"x_q\"])\n",
    "        self.eta_g = nn.Conv2d(h_dim, x_dim, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, u, sigma):\n",
    "        mu = self.eta_g(u)\n",
    "        return {\"loc\":mu, \"scale\":sigma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQN(nn.Module):\n",
    "    def __init__(self, x_dim, v_dim, r_dim, h_dim, z_dim, L=12, SCALE=4):\n",
    "        super(GQN, self).__init__()\n",
    "        self.L = L\n",
    "        self.h_dim = h_dim\n",
    "        self.SCALE = SCALE\n",
    "\n",
    "        self.phi = Representation(x_dim, v_dim, r_dim)\n",
    "        self.generator_core = GeneratorCore(v_dim, r_dim, z_dim, h_dim, self.SCALE)\n",
    "        self.inference_core = InferenceCore(x_dim, v_dim, r_dim, h_dim)\n",
    "\n",
    "        self.upsample   = nn.ConvTranspose2d(h_dim, h_dim, kernel_size=SCALE, stride=SCALE, padding=0)\n",
    "        self.downsample_x = nn.Conv2d(x_dim, x_dim, kernel_size=SCALE, stride=SCALE, padding=0)\n",
    "        self.downsample_u = nn.Conv2d(h_dim, h_dim, kernel_size=SCALE, stride=SCALE, padding=0)\n",
    "\n",
    "        # distribution\n",
    "        self.pi = Prior(z_dim, h_dim)\n",
    "        self.q = Inference(z_dim, h_dim)\n",
    "        self.g = Generator(x_dim, h_dim)\n",
    "\n",
    "    def forward(self, x, v, v_q, x_q, sigma):\n",
    "        batch_size, n_views, _, h, w = x.size()\n",
    "        \n",
    "        # merge batch and view dimensions.\n",
    "        _, _, *x_dims = x.size()\n",
    "        _, _, *v_dims = v.size()\n",
    "\n",
    "        x = x.view((-1, *x_dims))\n",
    "        v = v.view((-1, *v_dims))\n",
    "\n",
    "        # representation generated from input images and corresponding viewpoints\n",
    "        r = self.phi(x, v)\n",
    "\n",
    "        # seperate batch and view dimensions\n",
    "        _, *r_dims = r.size()\n",
    "        r = r.view((batch_size, n_views, *r_dims))\n",
    "\n",
    "        # sum over view representations\n",
    "        r = torch.sum(r, dim=1)\n",
    "\n",
    "        _, _, h, w = x.size()\n",
    "\n",
    "        # increase dimensions\n",
    "        v_q = v_q.view(batch_size, -1, 1, 1).repeat(1, 1, h//self.SCALE, w//self.SCALE)\n",
    "        \n",
    "        if r.size(2) != h//self.SCALE:\n",
    "            r = r.repeat(1, 1, h//self.SCALE, w//self.SCALE)\n",
    "\n",
    "        # reset hidden state\n",
    "        hidden_g = x_q.new_zeros((batch_size, self.h_dim, h//self.SCALE, w//self.SCALE))\n",
    "        hidden_i = x_q.new_zeros((batch_size, self.h_dim, h//self.SCALE, w//self.SCALE))\n",
    "\n",
    "        # reset cell state\n",
    "        cell_g = x_q.new_zeros((batch_size, self.h_dim, h//self.SCALE, w//self.SCALE))\n",
    "        cell_i = x_q.new_zeros((batch_size, self.h_dim, h//self.SCALE, w//self.SCALE))\n",
    "        \n",
    "        # reset u state\n",
    "        u = x.new_zeros((batch_size, self.h_dim, h, w))\n",
    "        \n",
    "        _x_q = self.downsample_x(x_q)\n",
    "\n",
    "        kls = 0\n",
    "        for _ in range(self.L):\n",
    "            z = self.q.sample({\"h_i\": hidden_i}, reparam=True)[\"z\"]\n",
    "            # kl divergence between posterior and prior\n",
    "            kl = KullbackLeibler(self.q, self.pi)\n",
    "            kl_tensor = kl.estimate({\"h_i\":hidden_i, \"h_g\":hidden_g})\n",
    "            kls += kl_tensor\n",
    "            # update state\n",
    "            _u = self.downsample_u(u)\n",
    "            hidden_i, cell_i = self.inference_core(_x_q, v_q, r, hidden_g, hidden_i, cell_i, _u)\n",
    "            hidden_g, cell_g, u = self.generator_core(z, v_q, r, hidden_g, cell_g, u)\n",
    "            \n",
    "        # sample reconstruction\n",
    "        x_q_rec = torch.clamp(self.g.sample_mean({\"u\": u, \"sigma\":sigma}), 0, 1)\n",
    "        # negative log-likelihood\n",
    "        nll = NLL(self.g)\n",
    "        nll_tensor = nll.estimate({\"u\":u, \"sigma\":sigma, \"x_q\": x_q})\n",
    "\n",
    "        return nll_tensor, kls, x_q_rec\n",
    "    \n",
    "    def generate(self, x, v, v_q):\n",
    "        batch_size, n_views, _, h, w = x.size()\n",
    "        \n",
    "        # merge batch and view dimensions.\n",
    "        _, _, *x_dims = x.size()\n",
    "        _, _, *v_dims = v.size()\n",
    "\n",
    "        x = x.contiguous().view((-1, *x_dims))\n",
    "        v = v.contiguous().view((-1, *v_dims))\n",
    "\n",
    "        # representation generated from input images and corresponding viewpoints\n",
    "        r = self.phi(x, v)\n",
    "\n",
    "        # seperate batch and view dimensions\n",
    "        _, *r_dims = r.size()\n",
    "        r = r.view((batch_size, n_views, *r_dims))\n",
    "\n",
    "        # sum over view representations\n",
    "        r = torch.sum(r, dim=1)\n",
    "\n",
    "        # increase dimensions\n",
    "        v_q = v_q.view(batch_size, -1, 1, 1).repeat(1, 1, h//self.SCALE, w//self.SCALE)\n",
    "        \n",
    "        if r.size(2) != h//self.SCALE:\n",
    "            r = r.repeat(1, 1, h//self.SCALE, w//self.SCALE)\n",
    "\n",
    "        # reset hidden state\n",
    "        hidden_g = x.new_zeros((batch_size, self.h_dim, h//self.SCALE, w//self.SCALE))\n",
    "\n",
    "        # reset cell state\n",
    "        cell_g = x.new_zeros((batch_size, self.h_dim, h//self.SCALE, w//self.SCALE))\n",
    "        \n",
    "        # reset r state\n",
    "        u = x.new_zeros((batch_size, self.h_dim, h, w))\n",
    "        \n",
    "        for _ in range(self.L):\n",
    "            z = self.pi.sample({\"h_g\": hidden_g})[\"z\"]\n",
    "            # update state\n",
    "            hidden_g, cell_g, u = self.generator_core(z, v_q, r, hidden_g, cell_g, u)\n",
    "            \n",
    "        x_q_hat = torch.clamp(self.g.sample_mean({\"u\": u, \"sigma\":sigma}), 0, 1)\n",
    "\n",
    "        return x_q_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data(x_data, v_data, seed=None):\n",
    "    random.seed(seed)\n",
    "    batch_size, m, *_ = x_data.size()\n",
    "\n",
    "    # sample random number of views\n",
    "    n_views = random.randint(2, m-1)\n",
    "\n",
    "    indices = torch.randperm(m)\n",
    "    representation_idx, query_idx = indices[:n_views], indices[n_views]\n",
    "\n",
    "    x, v = x_data[:, representation_idx], v_data[:, representation_idx]\n",
    "    x_q, v_q = x_data[:, query_idx], v_data[:, query_idx]\n",
    "    \n",
    "    return x, v, x_q, v_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# dataset translater\n",
    "# https://github.com/l3robot/gqn_datasets_translator\n",
    "\n",
    "# dataset directory\n",
    "train_data_dir = '/workspace/dataset/shepard_metzler_7_parts-torch/train'\n",
    "test_data_dir = '/workspace/dataset/shepard_metzler_7_parts-torch/test'\n",
    "\n",
    "# number of workers to load data\n",
    "num_workers = 0\n",
    "\n",
    "# log\n",
    "log_interval_num = 100\n",
    "save_interval_num = 100000\n",
    "dir_name = str(datetime.datetime.now())\n",
    "log_dir = '/workspace/logs/'+ dir_name\n",
    "os.mkdir(log_dir)\n",
    "os.mkdir(log_dir+'/models')\n",
    "os.mkdir(log_dir+'/runs')\n",
    "\n",
    "# tensorboardX\n",
    "writer = SummaryWriter(log_dir=log_dir+'/runs')\n",
    "\n",
    "batch_size = 36\n",
    "gradient_steps = 2*(10**6)\n",
    "\n",
    "train_dataset = ShepardMetzler(root_dir=train_data_dir, target_transform=transform_viewpoint)\n",
    "test_dataset = ShepardMetzler(root_dir=test_data_dir, target_transform=transform_viewpoint)\n",
    "\n",
    "# model settings\n",
    "xDim=3\n",
    "vDim=7\n",
    "rDim=256\n",
    "hDim=128\n",
    "zDim=64\n",
    "L=12\n",
    "SCALE = 4 # scale of image generation process\n",
    "\n",
    "# model\n",
    "gqn=GQN(xDim,vDim,rDim,hDim,zDim, L, SCALE).to(device)\n",
    "gqn = nn.DataParallel(gqn)\n",
    "\n",
    "# learning rate\n",
    "mu_i, mu_f = 5e-4, 5e-5\n",
    "\n",
    "# pixel variance\n",
    "sigma_i, sigma_f = 2.0, 0.7\n",
    "\n",
    "# initial value\n",
    "mu, sigma = mu_i, sigma_i\n",
    "\n",
    "optimizer = torch.optim.Adam(gqn.parameters(), lr=mu, betas=(0.9, 0.999))\n",
    "kwargs = {'num_workers':num_workers, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=36, shuffle=True, **kwargs)\n",
    "    \n",
    "x_data_test, v_data_test = next(iter(test_loader))\n",
    "\n",
    "# number of gradient steps\n",
    "s = 0\n",
    "while True:\n",
    "    for x_data, v_data in tqdm(train_loader):\n",
    "        x_data = x_data.to(device)\n",
    "        v_data = v_data.to(device)\n",
    "        x, v, x_q, v_q = arrange_data(x_data, v_data)\n",
    "        nll, kl, x_q_rec = gqn(x, v, v_q, x_q, sigma)\n",
    "        nll = nll.mean()\n",
    "        kl = kl.mean()\n",
    "        loss = nll + kl\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        writer.add_scalar('train_nll', nll, s)\n",
    "        writer.add_scalar('train_kl', kl, s)\n",
    "        writer.add_scalar('train_loss', loss, s)\n",
    "        \n",
    "        s += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # write logs to tensorboard\n",
    "            if s % log_interval_num == 0 or s == 1:\n",
    "                writer.add_image('train_ground_truth', x_q[:8], s)\n",
    "                writer.add_image('train_reconstruction', x_q_rec[:8], s)\n",
    "                \n",
    "                x_data_test = x_data_test.to(device)\n",
    "                v_data_test = v_data_test.to(device)\n",
    "                \n",
    "                x_test, v_test, x_q_test, v_q_test = arrange_data(x_data_test, v_data_test, seed=0)\n",
    "                nll_test, kl_test, x_q_rec_test = gqn(x_test, v_test, v_q_test, x_q_test, sigma)\n",
    "                x_q_hat_test = gqn.module.generate(x_test, v_test, v_q_test)\n",
    "                \n",
    "                nll_test = nll_test.mean()\n",
    "                kl_test = kl_test.mean()\n",
    "                loss_test = nll_test + kl_test\n",
    "                \n",
    "                writer.add_scalar('test_nll', nll_test, s)\n",
    "                writer.add_scalar('test_kl', kl_test, s)\n",
    "                writer.add_scalar('test_loss', loss_test, s)\n",
    "                writer.add_image('test_ground_truth', x_q_test[:8], s)\n",
    "                writer.add_image('test_reconstruction', x_q_rec_test[:8], s)\n",
    "                writer.add_image('test_generation', x_q_hat_test[:8], s)\n",
    "                \n",
    "            if s % save_interval_num == 0:\n",
    "                torch.save(gqn.state_dict(), log_dir + \"/models/model-{}.pt\".format(s))\n",
    "                \n",
    "            if s >= gradient_steps:\n",
    "                break\n",
    "\n",
    "            # Anneal learning rate\n",
    "            mu = max(mu_f + (mu_i - mu_f)*(1 - s/(1.6 * 10**6)), mu_f)\n",
    "            for group in optimizer.param_groups:\n",
    "                group[\"lr\"] = mu\n",
    "            # Anneal pixel variance\n",
    "            sigma = max(sigma_f + (sigma_i - sigma_f)*(1 - s/(2 * 10**5)), sigma_f)\n",
    "        \n",
    "    if s >= gradient_steps:\n",
    "        torch.save(gqn.state_dict(), log_dir + \"/models/model-final.pt\")\n",
    "        break\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
